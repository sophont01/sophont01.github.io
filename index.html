<!doctype html>
<html>
  <head>
	<!-- Global site tag (gtag.js) - Google Analytics
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-134998489-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());
	
	  gtag('config', 'UA-134998489-1');
	</script> -->

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
      	<style type="text/css">
	/* Stolen from Sergey and Jon Barron */
	  a {
	  color: #1772d0;
	  text-decoration:none;
	  }
	  a:focus, a:hover {
	  color: #f09228;
	  text-decoration:none;
	  }
	  body,td,th {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	        font-size: 15px
	  }
	  strong {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;
        	font-size: 14px
	  }
	  strongred {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	        color: 'red'
	        font-size: 14px
	  }
	  heading {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
	        font-size: 15px;
        	font-weight: 700
  	 }
	 </style>
    <script type="text/javascript" src="javascripts/hidebib.js"></script>
    <title>Saurabh Saini</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
	    <header>
			<center>
	     <em><h1 style="color:salmon; font-family:cursive; font-size:250%; ">Saurabh Saini</h1></em>
	     
	<img src="./data/images/profilepic_1.jpg" alt="profile pic" width="400px" height="300px"><br><br>
	<p><small>saurabh[dot]saini[at]research[dot]iiit[dot]ac[dot]in</small></p>
	<p>
		<a href="https://www.linkedin.com/in/saurabh0saini/">LinkedIn</a>&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://dblp.org/pers/hd/s/Saini:Saurabh">Dblp</a>&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.co.in/citations?user=OSZDITwAAAAJ&hl=en">Google Scholar</a>&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="./data/docs/RESUME_2024March.pdf">CV</a>&nbsp;&nbsp;&nbsp;&nbsp;
	</p>

		 </center>
      </header>
      <section>
	     <div style="background-image: url(./data/images/3_a.png); float:left; height:450px; width: 900px; opacity: 1;">  
		<p><br><h1 style="color:salmon;">&emsp;&emsp;About Me</h1></p>
		<center>
	      <p style="margin:50px;"><br>
	      I am finishing my Ph.D. (defense pending) under the guidance of <a href="https://faculty.iiit.ac.in/~pjn/">Prof. P. J. Narayanan</a> in <a href="http://cvit.iiit.ac.in">Center for Visual Information Technology (CVIT)</a> lab, a part of <a href="https://kcis.iiit.ac.in/">Kohli Center on Intelligent Systems (KCIS)</a>, at the <a href="https://www.iiit.ac.in/">International Institute of Information Technology-Hyderabad (IIIT-H) </a> India.
	      <br><br>
	      My general research interest is in the topics related to Computer Vision, Machine Learning and Optimization, especially applied to 2D or 3D reconstruction.
	      Currently I am working on <b>Inverse Rendering</b> problems. Specifically, I am focusing on Intrinsic Image Decomposition (separating object property based vs. light related components in an image) 
		  and Image Based Relighting (analyzing and editing image illumination).
	      <br><br>
	      Apart from this, I also like wandering in the domains of 3D modeling, registration, tracking, feature embedding, transfer learning, geometric learning, explainable AI etc. (the list grows ... :)
	      <br><br><br>
	      Personally, sometimes I enjoy sketching (graphite), reading (non-fiction) and writing (poems). 

	     <br><br></p>
	     </center>
	     </div>
	     &nbsp;
	     <p><br> <br><h1 style="color:salmon;">&emsp;&emsp;Publications</h1></p>
	     <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
	     <div style="float:left; height:500px; width:900px">
		     <hr>
		     <table align="center" border="0" cellspacing="0" cellpadding="20">

                <!-- CVPR 2024 My -->
                <tr>
                    <td width="33%" valign="top"><a href="./data/images/RSFNet.png"><img src="./data/images/RSFNet.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
                    <td width="67%" valign="top"><br>
                    <p>
                        <!-- <a id="RSFNet_pdf" href="./data/docs/RSFNetFull_lowRes.pdf"> -->
                        <a id="RSFNet_pdf" href="./data/projects/RSFNet">
                        <heading>Specularity Factorization for Low Light Enhancement</heading>
                        </a>
                        <br>
                        Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=3HKjt_IAAAAJ&hl=en&oi=ao">P J Narayanan</a><br>
        
                        <em>CVPR (to appear), 2024</em>
                    </p>
                    <div class="paper" id="RSFNet">
                        <a href="javascript:toggleblock('RSFNet_abs')" class="toggleblock">abstract</a> |
                        <a shape="rect" href="javascript:togglebib('RSFNet')" class="togglebib">bibtex</a> |
                        <abs><p align="justify" style="font-size:12px;"> <i id="RSFNet_abs">
                            Low Light Enhancement (LLE) is an important step to enhance images captured with insufficient light.
                            Several local and global methods have been proposed over the years for this problem. Decomposing the
                            image into multiple factors using an appropriate property is the first step in many LLE methods. In this paper,
                            we present a new additive factorization that treats images to be composed of multiple latent specular components
                            that can be estimated by modulating the sparsity during decomposition. We propose a model-driven learnable RSFNet
                            framework to estimate these factors by unrolling the optimization into network layers. The factors are interpretable
                            by design and can be manipulated directly for different tasks. We train our LLE system in a {\em zero-reference} manner
                            without the need for any paired or unpaired supervision. Our system improves the state-of-the-art performance on
                            standard benchmarks and achieves better generalization on multiple other datasets. The specularity factors
                            can supplement other task specific fusion networks by inducing prior information for enhancement
                            tasks like deraining, deblurring and dehazing with negligible overhead as shown in the paper.
                        </i></p></abs>
<pre>
<code>
    @inproceedings{
        sainiRSFNet,
        title={Specularity Factorization for Low Light Enhancement},
        author={Saurabh Saini and P J Narayanan},
        booktitle={(under review)},
        year={2024},
        }
</code>
</pre>
                    </div>
                    </td>
                </tr>



                <!-- NeurIPS AVANI -->
                <tr>
					<td width="33%" valign="top"><a href="./data/images/conceptDistil.png"><img src="./data/images/conceptDistil.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
					<p>
						<a id="ConceptDistil_pdf" href="./data/docs/ConceptDistillation.pdf">
						<heading>Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement</heading>
						</a>
						<br>
						<a href="https://avani17101.github.io/">Avani Gupta</a>, Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=3HKjt_IAAAAJ&hl=en&oi=ao">P J Narayanan</a><br>
		
						<em>NeurIPS, 2023 </em>(Poster)
					</p>
					<div class="paper" id="ConDistil">
						<a href="javascript:toggleblock('ConDistil_abs')" class="toggleblock">abstract</a> |
						<a shape="rect" href="javascript:togglebib('ConDistil')" class="togglebib">bibtex</a> |
						<a href="https://avani17101.github.io/Concept-Distilllation/">webpage</a> |
						<a href="https://github.com/avani17101/CD/">code</a> | 
						<a href="https://arxiv.org/abs/2311.15303">arxiv</a> |
                        <a href="https://github.com/avani17101/CD/">poster</a> |
						<a href="https://youtu.be/pg81pUgAE2I">video</a> 	
						<abs><p align="justify" style="font-size:12px;"> <i id="ConDistil_abs">
                            Humans use abstract concepts for understanding instead of hard features.
                            Recent interpretability research has focused on human-centered concept explanations of neural networks.
                            Concept Activation Vectors (CAVs) estimate a model's sensitivity and possible biases to a given concept.
                            In this paper, we extend CAVs from post-hoc analysis to ante-hoc training in order to reduce model bias through
                            fine-tuning using an additional Concept Loss. Concepts were defined on the final layer of the network in the past.
                            We generalize it to intermediate layers using class prototypes. This facilitates class learning in the last
                            convolution layer, which is known to be most informative. We also introduce Concept Distillation to create
                            richer concepts using a pre-trained knowledgeable model as the teacher. Our method can sensitize or desensitize
                            a model towards concepts. We show applications of concept-sensitive training to debias several classification problems.
                            We also use concepts to induce prior knowledge into IID, a reconstruction problem. Concept-sensitive training
                            can improve model interpretability, reduce biases, and induce prior knowledge. Please visit this https URL for code and more details.
						</i></p></abs>
<pre>
<code>
    @inproceedings{
        gupta2023concept,
        title={Concept Distillation: Leveraging Human-Centered Explanations for Model Improvement},
        author={Avani Gupta and Saurabh Saini and P J Narayanan},
        booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
        year={2023},
        url={https://openreview.net/forum?id=arkmhtYLL6}
        }
</code>
</pre>
					</div>
					</td>
				</tr>


				<!-- CVPR RAHUL -->
				<tr>
					<td width="33%" valign="top"><a href="./data/images/ISRF.png"><img src="./data/images/ISRF.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
					<p>
						<a id="ISRF_pdf" href="./data/docs/ISRF.pdf">
						<heading>Interactive Segmentation of Radiance Fields</heading>
						</a>
						<br>
						<a href="https://rahulgoel.xyz/">Rahul Goel</a>, <a href="https://dhawal1939.github.io/">Sirikonda Dhawal</a> Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=3HKjt_IAAAAJ&hl=en&oi=ao">P J Narayanan</a><br>
		
						<em>CVPR, 2023 </em>(Poster)
					</p>
					<div class="paper" id="ISRF">
						<a href="javascript:toggleblock('ISRF_abs')" class="toggleblock">abstract</a> |
						<a shape="rect" href="javascript:togglebib('ISRF')" class="togglebib">bibtex</a> |
						<a href="https://rahul-goel.github.io/isrf/">webpage</a> |
						<a href="https://github.com/rahul-goel/isrf_code">code</a> | 
						<a href="https://arxiv.org/abs/2212.13545">arxiv</a> |
						<a href="https://www.youtube.com/watch?v=L9YtSe7JEG8">video</a> 	
						<abs><p align="justify" style="font-size:12px;"> <i id="ISRF_abs">
						Radiance Fields (RF) are popular to represent casually-captured scenes for new view synthesis and several applications beyond it.
						 Mixed reality on personal spaces needs understanding and manipulating scenes represented as RFs, with semantic segmentation of objects
						 as an important step. Prior segmentation efforts show promise but do not scale to complex objects with diverse appearance.
						 We present the ISRF method to interactively segment objects with fine structure and appearance. Nearest neighbor feature matching
						 using distilled semantic features identifies high-confidence seed regions. Bilateral search in a joint spatio-semantic space grows
						 the region to recover accurate segmentation. We show state-of-the-art results of segmenting objects from RFs and compositing them to
						 another scene, changing appearance, etc., and an interactive segmentation tool that others can use.
						</i></p></abs>
<pre>
<code>
@inproceedings{isrfgoel2023,
title={{Interactive Segmentation of Radiance Fields}}, 
author={Goel, Rahul and Sirikonda, Dhawal and Saini, Saurabh and Narayanan, P.J.},
year={2023},
booktitle = {{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}},
}
</code>
</pre>
					</div>
					</td>
				</tr>



	
				<!-- ICVGIP Saurabh -->
				<tr>
					<td width="33%" valign="top"><a href="./data/images/QFSEF.png"><img src="./data/images/QFSEF.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
					<p>
						<a id="QFSEF_pdf" href="./data/docs/QFSEF.pdf">
						<heading>Quaternion Factorized Simulated Exposure Fusion for Low Light Image Enhancement</heading>
						</a>
						<br>
						Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=3HKjt_IAAAAJ&hl=en&oi=ao">P J Narayanan</a><br>
						<em>ICVGIP, 2022 </em><b>(Oral)</b>
					</p>
					<div class="paper" id="QFSEF">
						<a href="javascript:toggleblock('QFSEF_abs')" class="toggleblock">abstract</a> |
						<a shape="rect" href="javascript:togglebib('QFSEF')" class="togglebib">bibtex</a> |
						<a href="">webpage</a> |
						<a href="https://github.com/sophont01/QFSEF">code</a> | 
						<a href="./data/docs/QFSEF_poster.pdf">poster</a> 	
						<abs><p align="justify" style="font-size:12px;"> <i id="QFSEF_abs">
						Image Fusion maximizes the visual information at each pixel location by merging content
						from multiple images in order to produce an enhanced image. 
						Exposure Fusion, specifically, fuses a bracketed exposure stack of poorly lit images
						to generate a properly illuminated image. Given a single input image, 
						exposure fusion can still be employed on a ‘simulated’ exposure stack,
						leading to direct single image contrast and low-light enhancement.
						In this work, we present a novel ‘Quaternion Factorized Simulated Exposure Fusion’ (QFSEF)
						method by factorizing an input image into multiple illumination consistent layers.
						To this end, we use an iterative sparse matrix factorization scheme by representing
						the image as a two-dimensional pure quaternion matrix. Theoretically, our representation
						is based on the dichromatic reflection model and accounts for the two scene illumination
						characteristics by factorizing each progressively generated image into separate
						specular and diffuse components. We empirically prove the advantages of our
						factorization scheme over other exposure simulation methods by using it for the
						low-light image enhancement task.
						</i></p></abs>
<pre>
<code>
@inproceedings{sainiQFSEF22,
author = {Saini, Saurabh and Narayanan, P. J.},
title = {Quaternion Factorized Simulated Exposure Fusion for Low Light Image Enhancement},
year = {2022},
doi = {10.1145/3571600.3571604},
booktitle = {Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing},
series = {ICVGIP '22}}
</code>
</pre>
					</div>
					</td>
				</tr>


				<!-- ICVGIP Avani -->
				<tr>
					<td width="33%" valign="top"><a href="./data/images/tcavIID.png"><img src="./data/images/tcavIID.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
					<p>
						<a id="tcavIID_pdf" href="./data/docs/tcavIID.pdf">
						<heading>Interpreting Intrinsic Image Decomposition using Concept Activations</heading>
						</a>
						<br>
						<a href="https://avani17101.github.io/">Avani Gupta</a>, Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=3HKjt_IAAAAJ&hl=en&oi=ao">P J Narayanan</a><br>
						<em>ICVGIP, 2022 </em><b>(Oral) <p style="color:orange">*****<a href="https://events.iitgn.ac.in/2022/icvgip/best_paper_awards.html">[Best Paper Award]</a>*****</p></b>
					</p>
					<div class="paper" id="tcavIID">
						<a href="javascript:toggleblock('tcavIID_abs')" class="toggleblock">abstract</a> |
						<a shape="rect" href="javascript:togglebib('tcavIID')" class="togglebib">bibtex</a> |
						<a href="https://avani17101.github.io/Concept-Sensitivity-Metric/">webpage</a> |
						<a href="https://github.com/avani17101/CSM">code</a> | 
						<a href="https://avani17101.github.io/Concept-Sensitivity-Metric/Poster.pdf">poster</a>	|
                        <a href="https://avani17101.github.io/Concept-Sensitivity-Metric/supplementary.pdf">supplementary</a>
						<abs><p align="justify" style="font-size:12px;"> <i id="tcavIID_abs">
							Evaluation of ill-posed problems like Intrinsic Image Decomposition (IID) is challenging.
							IID involves decomposing an image into its constituent illumination-invariant Reflectance (R) and albedo-invariant Shading (S) components.
							Contemporary IID methods use Deep Learning models and require large datasets for training.
							The evaluation of IID is carried out on either synthetic Ground Truth images or sparsely annotated natural images.
							A scene can be split into reflectance and shading in multiple, valid ways.
							Comparison with one specific decomposition in the ground-truth images used by current IID evaluation metrics like LMSE, MSE, DSSIM, WHDR, SAW AP %, etc. is inadequate.
							Measuring R-S disentanglement is a better way to evaluate the quality of IID.
							Inspired by ML interpretability methods, we propose Concept Sensitivity Metrics (CSM) that directly measure disentanglement using sensitivity to relevant concepts.
						</i></p></abs>
<pre>
<code>
@inproceedings{guptaIID22,
author = {Gupta, Avani and Saini, Saurabh and Narayanan, P. J.},
title = {Interpreting Intrinsic Image Decomposition using Concept Activations},
year = {2022},
doi = {10.1145/3571600.3571603},
booktitle = {Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing},
series = {ICVGIP '22}}
</code>
</pre>
					</div>
					</td>
				</tr>

				<!-- ICVGIP Rahul -->
				<tr>
					<td width="33%" valign="top"><a href="./data/images/styleTRF.png"><img src="./data/images/styleTRF.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
					<p>
						<a id="styleTRF_pdf" href="./data/docs/styleTRF.pdf">
						<heading>StyleTRF: Stylizing Tensorial Radiance Fields</heading>
						</a>
						<br>
						<a href="https://rahulgoel.xyz/">Rahul Goel</a>, <a href="https://dhawal1939.github.io/">Sirikonda Dhawal</a> Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=3HKjt_IAAAAJ&hl=en&oi=ao">P J Narayanan</a><br>
						<em>ICVGIP, 2022 </em><b>(Spotlight) </b>
					</p>
					<div class="paper" id="styleTRF">
						<a href="javascript:toggleblock('styleTRF_abs')" class="toggleblock">abstract</a> |
						<a shape="rect" href="javascript:togglebib('styleTRF')" class="togglebib">bibtex</a> |
						<a href="https://rahul-goel.github.io/StyleTRF/">webpage</a> |
						<a href="https://arxiv.org/abs/2212.09330">arxiv</a> |
						<a href="./data/docs/styleTRF_poster.pdf">poster</a> |	
                        <a href="https://www.youtube.com/watch?v=K7OrKWPmxQM">video</a>
						<abs><p align="justify" style="font-size:12px;"> <i id="styleTRF_abs">
							Stylized view generation of scenes captured casually using a camera has received much attention recently.
							The geometry and appearance of the scene are typically captured as neural point sets or neural radiance fields in the previous work.
							An image stylization method is used to stylize the captured appearance by training its network jointly or iteratively with the structure capture network.
							The state-of-the-art SNeRF method trains the NeRF and stylization network in an alternating manner. These methods have high training time and require joint optimization.
							In this work, we present StyleTRF, a compact, quick-to-optimize strategy for stylized view generation using TensoRF.
							The appearance part is finetuned using sparse stylized priors of a few views rendered using the TensoRF representation for a few iterations.
							Our method thus effectively decouples style-adaption from view capture and is much faster than the previous methods. We show state-of-the-art results on several scenes used for this purpose.
						</i></p></abs>
<pre>
<code>
@inproceedings{goel2022styletrf,
author = {Goel, Rahul and Sirikonda, Dhawal and Saini, Saurabh and Narayanan, P. J.},
title = {StyleTRF: Stylizing Tensorial Radiance Fields},
year = {2022},
doi = {10.1145/3571600.3571643},
booktitle = {Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing},
series = {ICVGIP '22}}
</code>
</pre>
					</div>
					</td>
				</tr>

				<!-- ICCV -->
				<tr>
					<td width="33%" valign="top"><a href="./data/images/SNV.png"><img src="./data/images/SNV.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
							<p>
				<a id="ICCVsnv_pdf" href="./data/docs/snv_iccv2021.pdf">
				<heading>Learning to Stylize Novel Views</heading>
				</a>
				<br>
				<a href="https://hhsinping.github.io/">Hsin-Ping Huang</a>, <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>, Saurabh Saini, 
				<a href="https://scholar.google.com/citations?user=hdQhiFgAAAAJ&hl=en">Maneesh Singh</a>, <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a><br>
				<em>ICCV, 2021 (Poster)</em>
						</p>
						<div class="paper" id="ICCVsnv">
						<a href="javascript:toggleblock('ICCVsnv_abs')">abstract</a> |
						<a shape="rect" href="javascript:togglebib('ICCVsnv')" class="togglebib">bibtex</a> |
						<a href="https://hhsinping.github.io/3d_scene_stylization/">webpage</a> |
						<a href="https://github.com/hhsinping/stylescene">code</a> | 
						<a href="https://arxiv.org/abs/2105.13509">arxiv (+ supplementary)</a> 	
				<p align="justify" style="font-size:12px;"> <i id="ICCVsnv_abs">
					We tackle a 3D scene stylization problem — generating stylized images of a scene from arbitrary novel views given
					a set of images of the same scene and a reference image	of the desired style as inputs. Direct solution of combining
					novel view synthesis and stylization approaches lead to results that are blurry or not consistent across different views.
					We propose a point cloud-based method for consistent 3D	scene stylization. First, we construct the point cloud by 
					back-projecting the image features to the 3D space. Second, we develop point cloud aggregation modules to gather
					the style information of the 3D scene, and then modulate the features in the point cloud with a linear transformation
					matrix. Finally, we project the transformed features to 2D space to obtain the novel views. Experimental results on
					two diverse datasets of real-world scenes validate that our	method generates consistent stylized novel view synthesis
					results against other alternative approaches.
						</i></p>
<pre><code>
@article{Huang2021LearningTS,
title={Learning to Stylize Novel Views},
author={Hsin-Ping Huang and Hung-Yu Tseng and Saurabh Saini and Maneesh Kumar Singh and Ming-Hsuan Yang},
journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
year={2021},
pages={13849-13858}}
</code></pre>
					</div>
					</td>
				</tr>
				

				<!-- IJCV -->
				<tr>
					<td width="33%" valign="top"><a href="./data/images/sp10.png"><img src="./data/images/sp10.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
						  <p>
				<a id="IJCVSPIID_pdf" href="./data/docs/1902.03830.pdf">
				<heading>Semantic Hierarchical Priors for Intrinsic Image Decomposition</heading>
				</a>
				<br>
				Saurabh Saini,  <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
				<em>Arxiv, 2019</em>
					  </p>
					  <div class="paper" id="IJCVSPIID">
						<a href="javascript:toggleblock('IJCVSPIID_abs')">abstract</a> |
						<a shape="rect" href="javascript:togglebib('IJCVSPIID')" class="togglebib">bibtex</a> | 
					<a href="https://arxiv.org/abs/1902.03830">arxiv</a>	
				<p align="justify" style="font-size:12px;"> <i id="IJCVSPIID_abs">
				Intrinsic Image Decomposition (IID) is a challenging and interesting computer vision problem with various applications in several fields. We present novel
				semantic priors and an integrated approach for single image IID that involves analyzing image at three hierarchical context levels. Local context priors capture
				scene properties at each pixel within a small neighbourhood. Mid-level context priors encode object level semantics. Global context priors establish correspondences at the scene level. 
				Our semantic priors are designed on both fixed and flexible regions, using selective search method and Convolutional Neural Network features. 
				Our IID method is an iterative multistage optimization scheme and consists of two complementary formulations: L2 smoothing for shading and L1 sparsity for reflectance. Experiments and analysis of our
				method indicate the utility of our semantic priors and structured hierarchical analysis in an IID framework. We compare our method with other contemporary IID
				solutions and show results with lesser artifacts. Finally, we highlight that proper choice and encoding of prior knowledge can produce competitive results even when
				compared to end-to-end deep learning IID methods, signifying the importance of such priors. We believe that the insights and techniques presented in this paper would be useful in the future IID research.
						</i></p>
<pre><code>
@article{Saini2019SemanticHP,
title={Semantic Hierarchical Priors for Intrinsic Image Decomposition},
author={Saurabh Saini and P. J. Narayanan},
journal={ArXiv},
year={2019},
volume={abs/1902.03830}}
</code></pre>
					</div>
					</td>
				</tr>

				
				<!-- Aakash arxiv -->
				<tr>
					<td width="33%" valign="top"><a href="./data/images/matNN.jpg"><img src="./data/images/matNN.jpg" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
					<td width="67%" valign="top"><br>
						<p>
							<a id="NNrendering_pdf" href="./data/docs/1908.09530.pdf"><heading>Neural Rendering for Material Visualization</heading></a><br>
							Aakash KT, <a href="https://researchweb.iiit.ac.in/~parikshit.sakurikar/">Parikshit Sakurikar</a>, Saurabh Saini, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
							<em>Siggraph Asia, 2019 (Technical Briefs)</em>
						</p>
						<div class="paper" id="NNrendering">
							<a href="javascript:toggleblock('NNrendering_abs')">abstract</a> |
							<a shape="rect" href="javascript:togglebib('NNrendering')" class="togglebib">bibtex</a> |
							<a href="https://aakashkt.github.io/neural-renderer-material-visualization.html">webpage</a> | 
							<a href="https://github.com/AakashKT/NeuralMaterialVisualization">code</a> |
						<a href="http://arxiv.org/abs/1908.09530">arxiv</a>	
					<p align="justify" style="font-size:12px;"> <i id="NNrendering_abs">
						Photo realism in computer generated imagery is crucially dependent on how well an artist is able to recreate real-world materials in the scene.
						The workflow for material modeling and editing typically involves manual tweaking of material parameters and uses a standard path tracing engine for visual feedback.
						A lot of time may be spent in iterative selection and rendering of materials at an appropriate quality.
						In this work, we propose a convolutional neural network based workflow which quickly generates high-quality ray traced material visualizations on a shaderball.
						Our novel architecture allows for control over environment lighting and assists material selection along with the ability to render spatially-varying materials.
						Additionally, our network enables control over environment lighting which gives an artist more freedom and provides better visualization of the rendered material.
						Comparison with state-of-the-art denoising and neural rendering techniques suggests that our neural renderer performs faster and better.
						We provide a interactive visualization tool and release our training dataset to foster further research in this area.
							</i></p>
<pre><code>@ARTICLE{2019arXiv190809530K,
author = {KT, Aakash and Sakurikar, Parikshit and Saini, Saurabh and Narayanan, P. J.},
title = "{A Flexible Neural Renderer for Material Visualization}",
journal = {arXiv e-prints},
keywords = {Computer Science - Graphics},
year = "2019",
month = "Aug",
archivePrefix = {arXiv},
eprint = {1908.09530},
primaryClass = {cs.GR},}
</code></pre>
						</div>


					</td>
				</tr>
				<tr>
		
		     
		<!-- BMVC -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/blockDiagram.png"><img src="./data/images/blockDiagram.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="http://bmvc2018.org/contents/papers/0796.pdf" id="SF">
			<heading>Semantic Priors for Intrinsic Image Decomposition</heading></a><br>
			Saurabh Saini, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan </a><br>
			<em>BMVC</em>, 2018 <b> (Oral) <p style="color:orange">*****<a href="http://bmvc2018.org/awards.html">[Best Industrial Paper, Honourable Mention]</a>*****</p></b>
        	      </p>
	              <div class="paper" id="sf">
        	        <a href="javascript:toggleblock('sf_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('sf')" class="togglebib">bibtex</a> | 
					<a href="./data/docs/0796.pdf">pdf</a> | 
					<a href="./data/docs/0796_supplementary.pdf">supplementary</a>
        	        <p align="justify" style="font-size:12px;"> <i id="sf_abs">Intrinsic Image Decomposition (IID) is a challenging and interesting computer vision
			problem with various applications in several fields. We present novel semantic priors and
			an integrated approach for single image IID that involves analyzing image at three hierarchical context levels. Local context priors capture scene properties at each pixel within a
			small neighborhood. Mid-level context priors encode object level semantics. Global context priors establish correspondences at the scene level. Our semantic priors are designed
			on both fixed and flexible regions, using selective search method and Convolutional Neural Network features. Experiments and analysis of our method indicate the utility of our
			weak semantic priors and structured hierarchical analysis in an IID framework. We compare our method with the current state-of-the-art and show results with lesser artifacts.
			Finally, we highlight that proper choice and encoding of prior knowledge can produce
			competitive results compared to end-to-end deep learning IID methods, signifying the
			importance of such priors. We believe that the insights and techniques presented in this
			paper would be useful in the future IID research.</i></p>
<pre><code>@inproceedings{DBLP:conf/bmvc/SainiN18,
author    = {Saurabh Saini and P. J. Narayanan},
title     = {Semantic Priors for Intrinsic Image Decomposition},
booktitle = {British Machine Vision Conference 2018, {BMVC} 2018, Northumbria University, Newcastle, UK, September 3-6, 2018},
pages     = {154},
year      = {2018},
url       = {http://bmvc2018.org/contents/papers/0796.pdf},}
</code></pre>
	            </div>
        	    </td>
	        </tr>

		<!-- WACV 2018 -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/3dcap.png"><img src="./data/images/3dcap.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="https://ieeexplore.ieee.org/document/8354153" id="WACV18_pdf">
			<heading>Human Shape Capture and Tracking at Home.</heading></a><br>
		<a href="https://researchweb.iiit.ac.in/~gaurav.mishra/">Gaurav Mishra</a>, Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=pDvg7X4AAAAJ&hl=en">Kiran Varanasi</a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>WACV</em>, 2018  <b> (Oral) </b> 
        	      </p>
	              <div class="paper" id="WACV18">
        	        <a href="javascript:toggleblock('WACV18_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('WACV18')" class="togglebib">bibtex</a> | 
			<a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/smpltracking">webpage</a> |
			<a href="./data/docs/08354153.pdf">pdf</a> | 
			<a href="https://www.youtube.com/watch?v=DNqn1O12ICI">supplementary</a> | 
			<a href="https://www.youtube.com/watch?v=mF6nqpwuA9E&t=6s">presentation</a>

			<p align="justify" style="font-size:12px;"> <i id="WACV18_abs">
			Human body tracking typically requires specialized capture set-ups. Although pose tracking is available in consumer devices like Microsoft Kinect, it is restricted to stick
			figures visualizing body part detection. In this paper, we propose a method for full 3D human body shape and motion capture of arbitrary movements from the depth channel
			of a single Kinect, when the subject wears casual clothes. We do not use the RGB channel or an initialization procedure that requires the subject to move around in front of the
			camera. This makes our method applicable for arbitrary clothing textures and lighting environments, with minimal subject intervention. Our method consists of 3D surface
			feature detection and articulated motion tracking, which is regularized by a statistical human body model. We also propose the idea of a Consensus Mesh (CMesh) which is the
			3D template of a person created from a single view point. We demonstrate tracking results on challenging poses and argue that using CMesh along with statistical body models
			can improve tracking accuracies. Quantitative evaluation of our dense body tracking shows that our method has very little drift which is improved by the usage of CMesh.
        	        </i></p>
<pre><code>@inproceedings{DBLP:conf/wacv/MishraSVN18,
author    = {Gaurav Mishra and Saurabh Saini and Kiran Varanasi and P. J. Narayanan},
title     = {Human Shape Capture and Tracking at Home},
booktitle = {2018 {IEEE} Winter Conference on Applications of Computer Vision, {WACV} 2018, Lake Tahoe, NV, USA, March 12-15, 2018},
pages     = {390--399},
year      = {2018},
doi       = {10.1109/WACV.2018.00049},}
</code></pre>
	            </div>
        	    </td>
	        </tr>


		<!-- ICVGIP -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/6_Outdoor3.jpg"><img src="./data/images/6_Outdoor3.jpg" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="./data/docs/a88-saini.pdf" id="ICVGIP16_pdf">
			<heading>Intrinsic image decomposition using focal stacks.</heading></a><br>
			Saurabh Saini, <a href="https://researchweb.iiit.ac.in/~parikshit.sakurikar/">Parikshit Sakurikar </a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>ICVGIP</em>, 2016  <b> (Oral) </b>
        	      </p>
	              <div class="paper" id="ICVGIP16">
        	        <a href="javascript:toggleblock('ICVGIP16_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('ICVGIP16')" class="togglebib">bibtex</a> |
			<a href="https://github.com/sophont01/fStackIID">code</a> |
			<a href="./data/docs/a88-saini.pdf">pdf</a> | 
			<a href="./data/docs/paperId_218_supp.pdf">supplementary</a>
			<p align="justify" style="font-size:12px;"> <i id="ICVGIP16_abs">
			In this paper, we presents a novel method (RGBF-IID) for intrinsic image decomposition of a wild scene without any restrictions on the complexity, illumination or scale of the
			image. We use focal stacks of the scene as input. A focal stack captures a scene at varying focal distances. Since focus depends on distance to the object, this representation has
			information beyond an RGB image towards an RGBD image with depth. We call our representation an RGBF image to highlight this. We use a robust focus measure and generalized
		       	random walk algorithm to compute dense probability maps across the stack. These maps are used to define sparse local and global pixel neighbourhoods, adhering to the structure
		       	of the underlying 3D scene. We use these neighbourhood correspondences with standard chromaticity assumptions as constraints in an optimization system. We present
			our results on both indoor and outdoor scenes using manually captured stacks of random objects under natural as well as artificial lighting conditions. We also test our system on
			a larger dataset of synthetically generated focal stacks from NYUv2 and MPI Sintel datasets and show competitive performance against current state-of-the-art IID methods that
			use RGBD images. Our method provides a strong evidence for the potential of RGBF modality in place of RGBD in computer vision
			</i></p>
<pre><code>@inproceedings{DBLP:conf/icvgip/SainiSN16,
author    = {Saurabh Saini and Parikshit Sakurikar and P. J. Narayanan},
title     = {Intrinsic image decomposition using focal stacks},
booktitle = {Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing, {ICVGIP} 2016, Guwahati, Assam, India, December 18-22, 2016},
pages     = {88:1--88:8},
year      = {2016},
url       = {https://doi.org/10.1145/3009977.3010046},
doi       = {10.1145/3009977.3010046},}
</code></pre>
	            </div>
        	    </td>
	        </tr>
		

		<!-- ICVGIP Aditya -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/tag2vec.png"><img src="./data/images/tag2vec.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="./data/docs/a94-singh.pdf" id="ICVGIP16A_pdf">
			<heading>Learning to hash-tag videos with Tag2Vec</heading></a><br>
			<a href="https://cvit.iiit.ac.in/people/ms-by-research/ms-students/aditya-singh">Aditya Singh</a>, Saurabh Saini, <a href="http://rajvishah.weebly.com/">Rajvi Shah</a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>ICVGIP</em>, 2016  <b> (Oral) </b>
        	      </p>
	              <div class="paper" id="ICVGIP16A">
        	        <a href="javascript:toggleblock('ICVGIP16A_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('ICVGIP16A')" class="togglebib">bibtex</a> | 
			<a href="https://arxiv.org/abs/1612.04061">arxiv</a>
			<p align="justify" style="font-size:12px;"> <i id="ICVGIP16A_abs">
			User-given tags or labels are valuable resources for semantic understanding of visual media such as images and videos. Recently, a new type of labeling mechanism known as hash-tags
		       	have become increasingly popular on social media sites. In this paper, we study the problem of generating relevant and useful hash-tags for short video clips. Traditional data-driven
		       	approaches for tag enrichment and recommendation use direct visual similarity for label transfer and propagation. We attempt to learn a direct low-cost mapping from
		       	video to hash-tags using a two step training process. We first employ a natural language processing (NLP) technique, skip-gram models with neural network training to
			learn a low-dimensional vector representation of hash-tags (Tag2Vec) using a corpus of ∼ 10 million hash-tags. We then train an embedding function to map video features to
 			the low-dimensional Tag2vec space. We learn this embedding for 29 categories of short video clips with hash-tags. A query video without any tag-information can then be 
			directly mapped to the vector space of tags using the learned embedding and relevant tags can be found by performing a simple nearest-neighbor retrieval in the Tag2Vec space. We
			validate the relevance of the tags suggested by our system qualitatively and quantitatively with a user study.
			</i></p>
<pre><code>@inproceedings{DBLP:conf/icvgip/SinghSSN16,
author    = {Aditya Singh and Saurabh Saini and Rajvi Shah and P. J. Narayanan},
title     = {Learning to hash-tag videos with Tag2Vec},
booktitle = {Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing, {ICVGIP} 2016, Guwahati, Assam, India, December 18-22, 2016},
pages     = {94:1--94:8},
year      = {2016},
url       = {https://doi.org/10.1145/3009977.3010035},
doi       = {10.1145/3009977.3010035},}
</code></pre>
	            </div>
        	    </td>
	        </tr>


		<!-- GCPR 2016 -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/gcpr.png"><img src="./data/images/gcpr.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="https://cvit.iiit.ac.in/images/ConferencePapers/2016/GCPR2016.pdf" id="GCPR16_pdf">
			<heading>From Traditional to Modern : Domain Adaptation for Action Classification in Short Social Video Clips</heading></a><br>
			<a href="https://cvit.iiit.ac.in/people/ms-by-research/ms-students/aditya-singh">Aditya Singh</a>, Saurabh Saini, <a href="http://rajvishah.weebly.com/">Rajvi Shah</a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>GCPR</em>, 2016 <b> (Oral) </b>
        	      </p>
	              <div class="paper" id="GCPR16">
        	        <a href="javascript:toggleblock('GCPR16_abs')">abstract</a> | 
                	<a shape="rect" href="javascript:togglebib('GCPR16')" class="togglebib">bibtex</a> | 
			<a href="http://arxiv.org/abs/1610.05613">arxiv</a>
			<p align="justify" style="font-size:12px;"> <i id="GCPR16_abs">
			Short internet video clips like vines present a significantly wild distribution compared to traditional video datasets. 
			In this paper, we focus on the problem of unsupervised action classification in wild vines using traditional la- beled datasets.
		       	To this end, we use a data augmentation based simple domain adaptation strategy. We utilize semantic word2vec space as a common subspace to embed
		       	video features from both, labeled source domain and unlabled target domain. Our method incrementally augments the labeled source with target samples
		       	and iteratively modifies the embedding function to bring the source and target distributions together. Additionally, we utilize a multi-modal representation
		       	that incorporates noisy semantic information available in form of hash-tags. We show the effectiveness of this simple adaptation technique on a test set
		       	of vines and achieve notable improvements in performance.
			</i></p>
<pre><code>@inproceedings{DBLP:conf/dagm/SinghSSN16,
author    = {Aditya Singh and Saurabh Saini and Rajvi Shah and P. J. Narayanan},
title     = {From Traditional to Modern: Domain Adaptation for Action Classification in Short Social Video Clips},
booktitle = {Pattern Recognition - 38th German Conference, {GCPR} 2016, Hannover, Germany, September 12-15, 2016, Proceedings},
pages     = {245--257},
year      = {2016},
url       = {https://doi.org/10.1007/978-3-319-45886-1\_20},
doi       = {10.1007/978-3-319-45886-1\_20},}
</code></pre>
	            </div>
        	    </td>
	        </tr>
		  </table>


		<!-- PROJECTS -->
		<div style="float:left; height:500px; width:900px">
		  <p><br><br><h1 style="color:salmon;">&emsp;&emsp;Current Projects</h1></p>
		  <table align="center" border="0" cellspacing="0" cellpadding="20">
		    <!-- <tr>
	            <td width="33%" valign="top"><a href="./data/images/quatRes_3.png"><img src="./data/images/quatRes_3.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top"><br>
			    <p><heading>Recursuive Robust Retinex</heading> (under review) <br>
						Can we propose an improved formulation of Retinex Theory based on Robust Principal Componenet Analysis by factorizing an image into multiple sparse layers ? 
						This might be useful in various Image Based Relighting applications like: image relighting, white balancing, object compositing, low-light enhancement, shadow-specularity editing etc.
				</p>
				</td>
			</tr> -->
			<tr>
	            <td width="33%" valign="top"><a href="./data/images/QFSEF_res.png"><img src="./data/images/QFSEF_res.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top"><br>
			    <p><heading>Unrolled Robust Retinex for Low Light Enhancement</heading> (ongoing) <br>
						Can we build an unrolled architecture for Robust Retnex which can be self-supervised and employed for Low Light Enhancement ?
				</p>
				</td>
			</tr>
			<!-- <tr>
				<td width="33%" valign="top"><a href="./data/images/conceptDistil.png"><img src="./data/images/conceptDistil.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
				<td width="67%" valign="top"><br>
				<p><heading>Ad-hoc interpretations for model debiasing</heading> (under review) <br>
							The project aims using model interpretability techniques for ad-hoc model improvement by removing learning biases using a concept distillation framework.
					</p>
				</td>
			</tr> -->
			<tr>
				<td width="33%" valign="top"><a href="./data/images/IlluSpec.png"><img src="./data/images/IlluSpec.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
				<td width="67%" valign="top"><br>
				<p><heading>Illumination Spectra Separation</heading><br>
							The project aims at studying the feasibility of decomposing scene illumination based on the color spectra of the illuminanat,
							generating spatially varying illuination maps and a relit image.
					</p>
				</td>
			</tr>
			<!-- <tr>
				<td width="33%" valign="top"><a href="./data/images/matNN.jpg"><img src="./data/images/matNN.jpg" width="75%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
				<td width="67%" valign="top"><br><br>
						<p><heading>Neural Rendering for Material Visualization</heading><br>
							The project aims at real-time, raytraced and faithful representation of spatially varying and parametric 
							BRDF models on a shader ball for material visualization and selection.
						</p>
				</td>
			</tr> -->
			<!-- <tr>
				<td width="33%" valign="top"><a href="./data/images/3Dcategorization.png"><img src="./data/images/3Dcategorization.png" width="75%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
				<td width="67%" valign="top"><br><br>
						<p><heading>AutoML assisted 3D CNN analysis</heading><br>
							The project aims at analyzing different 3D models like PointNet, VoxelNet, Mutiview CNNs, GraphCNNs etc. 
							using AutoML techniques towards the goal of better understanding the strengths and weaknesses of 3D CNNs for various 3D problems.
						</p>
				</td>
			</tr>	 -->
		</table>


		<!-- EMPTY area at page bottom -->
		<!-- <p><h1 style="color:salmon;">&nbsp;</h1></p> -->
		<img align="right" src="./data/images/calvin_thinking.png" width="30%" style="border-style: none; margin-bottom: 50px; margin-top: 0px;">
		<!-- <img align="right" src="./data/images/calvin_reading.jpg" width="10%" style="border-style: none; margin-bottom: 50px; margin-top: 0px;"> -->
		</div>
		 


		 
    </section>
	<br style="line-height:1000px;"> 

    <script src="javascripts/scale.fix.js"></script>
    <script xml:space="preserve" language="JavaScript">	hideallbibs();	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('RSFNet_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('ConDistil_abs');	</script>
	<script xml:space="preserve" language="JavaScript">	hideblock('ISRF_abs');	</script>	
	<script xml:space="preserve" language="JavaScript">	hideblock('QFSEF_abs');	</script>
	<script xml:space="preserve" language="JavaScript">	hideblock('tcavIID_abs');	</script>
	<script xml:space="preserve" language="JavaScript">	hideblock('styleTRF_abs');	</script>
	<script xml:space="preserve" language="JavaScript">	hideblock('ICCVsnv_abs');	</script>
	<script xml:space="preserve" language="JavaScript">	hideblock('sf_abs');	</script>
	<script xml:space="preserve" language="JavaScript">	hideblock('NNrendering_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('IJCVSPIID_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('WACV18_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('ICVGIP16_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('ICVGIP16A_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('GCPR16_abs');	</script>
  </body>
</html>
