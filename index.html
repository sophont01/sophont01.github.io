<!doctype html>
<html>
  <head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-134998489-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());
	
	  gtag('config', 'UA-134998489-1');
	</script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
      	<style type="text/css">
	/* Stolen from Sergey and Jon Barron */
	  a {
	  color: #1772d0;
	  text-decoration:none;
	  }
	  a:focus, a:hover {
	  color: #f09228;
	  text-decoration:none;
	  }
	  body,td,th {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	        font-size: 15px
	  }
	  strong {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;
        	font-size: 14px
	  }
	  strongred {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;
	        color: 'red'
	        font-size: 14px
	  }
	  heading {
        	font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
	        font-size: 15px;
        	font-weight: 700
  	 }
	 </style>
    <script type="text/javascript" src="javascripts/hidebib.js"></script>
    <title>Saurabh Saini</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
	    <header>
	     <h1>Saurabh Saini</h1>
	     <center>
	<img src="./data/images/profile.jpg" alt="profile pic"><br><br>
	<p><small>saurabh[dot]saini[at]research[dot]iiit[dot]ac[dot]in</small></p>
	<p><small>
		<a href="https://www.linkedin.com/in/saurabh0saini/">LinkedIn</a>&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://dblp.org/pers/hd/s/Saini:Saurabh">Dblp</a>&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="https://scholar.google.co.in/citations?user=OSZDITwAAAAJ&hl=en">Google Scholar</a>&nbsp;&nbsp;&nbsp;&nbsp;
		<a href="">CV</a>&nbsp;&nbsp;&nbsp;&nbsp;
	</small></p>

		 </center>
      </header>
      <section>
	      <div style="background-image: url(./data/images/3_a.png); float:left; height:350px; width: 900px; opacity: 1;">
	      <center>
	      <p style="margin:50px;"><br>
	      I am a Ph.D. student under the guidance of <a href="https://faculty.iiit.ac.in/~pjn/">Prof. P. J. Narayanan</a> in <a href="http://cvit.iiit.ac.in">Center for Visual Information Technology (CVIT)</a> lab, a part of <a href="https://kcis.iiit.ac.in/">Kohli Center on Intelligent Systems (KCIS)</a>, at the <a href="https://www.iiit.ac.in/">International Institute of Information Technology-Hyderabad (IIIT-H) </a> India.
	      <br><br>
	      My general research interest is in the topics related to Computer Vision, Machine Learning and Optimization, especially applied to 2D or 3D reconstruction.
	      Currently I am working on Inverse Rendering and Inverse Light Transport problems. Specifically, I am focusing on Intrinsic Image Decomposition (separating object property based vs. light related components in an image).
	      <br><br>
	      Apart from this, I also like wandering in the domains of 3D human body modeling, registration and tracking, feature embedding, transfer learning and geometric learning.
	      <br><br>
	      Personally, I enjoy sketching (graphite), reading (non-fiction) and writing (poems). 

	     <br><br></p>
	     </center>
	     </div>
	     &nbsp;
	     <p><br> <br><h1>Publications</h1></p>
	     <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">-->
	     <div style="float:left; height:500px; width:900px">
		     <hr>
		     <table align="center" border="0" cellspacing="0" cellpadding="20">

		     <!-- IJCV -->
			 <tr>
	            <td width="33%" valign="top"><a href="./data/images/sp10.png"><img src="./data/images/sp10.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top"><br>
              		<p>
			<a id="IJCVSPIID_pdf">
			<heading>Semantic Hierarchical Priors for Intrinsic Image Decomposition</heading>
			</a>
			<br>
			Saurabh Saini,  <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>Under review IJCV</em>
        	      </p>
	              <div class="paper" id="IJCVSPIID">
        	        <a href="javascript:toggleblock('IJCVSPIID_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('IJCVSPIID')" class="togglebib">bibtex</a> | 
		        <a href="https://arxiv.org/abs/1902.03830">arxiv</a>	
			<p align="justify" style="font-size:12px;"> <i id="IJCVSPIID_abs">
			Intrinsic Image Decomposition (IID) is a challenging and interesting computer vision problem with various applications in several fields. We present novel
			semantic priors and an integrated approach for single image IID that involves analyzing image at three hierarchical context levels. Local context priors capture
			scene properties at each pixel within a small neighbourhood. Mid-level context priors encode object level semantics. Global context priors establish correspondences at the scene level. 
			Our semantic priors are designed on both fixed and flexible regions, using selective search method and Convolutional Neural Network features. 
			Our IID method is an iterative multistage optimization scheme and consists of two complementary formulations: L2 smoothing for shading and L1 sparsity for reflectance. Experiments and analysis of our
			method indicate the utility of our semantic priors and structured hierarchical analysis in an IID framework. We compare our method with other contemporary IID
			solutions and show results with lesser artifacts. Finally, we highlight that proper choice and encoding of prior knowledge can produce competitive results even when
			compared to end-to-end deep learning IID methods, signifying the importance of such priors. We believe that the insights and techniques presented in this paper would be useful in the future IID research.
        	        </i></p>
<pre><code>@inproceedings{DBLP:conf/bmvc/SainiN18,
author    = {Saurabh Saini and P. J. Narayanan},
title     = {Semantic Priors for Intrinsic Image Decomposition},
booktitle = {British Machine Vision Conference 2018, {BMVC} 2018, Northumbria University, Newcastle, UK, September 3-6, 2018},
pages     = {154},
year      = {2018},
url       = {http://bmvc2018.org/contents/papers/0796.pdf},}
</code></pre>
	            </div>
        	    </td>
		    </tr>
		
		     
		<!-- BMVC -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/blockDiagram.png"><img src="./data/images/blockDiagram.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="http://bmvc2018.org/contents/papers/0796.pdf" id="SF">
			<heading>Semantic Priors for Intrinsic Image Decomposition</heading></a><br>
			Saurabh Saini, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan </a><br>
			<em>BMVC</em>, 2018 <b> (Oral) <a href="http://bmvc2018.org/awards.html">[Best Industrial Paper, Honourable Mention]</a></b>
        	      </p>
	              <div class="paper" id="sf">
        	        <a href="javascript:toggleblock('sf_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('sf')" class="togglebib">bibtex</a>
        	        <p align="justify" style="font-size:12px;"> <i id="sf_abs">Intrinsic Image Decomposition (IID) is a challenging and interesting computer vision
			problem with various applications in several fields. We present novel semantic priors and
			an integrated approach for single image IID that involves analyzing image at three hierarchical context levels. Local context priors capture scene properties at each pixel within a
			small neighborhood. Mid-level context priors encode object level semantics. Global context priors establish correspondences at the scene level. Our semantic priors are designed
			on both fixed and flexible regions, using selective search method and Convolutional Neural Network features. Experiments and analysis of our method indicate the utility of our
			weak semantic priors and structured hierarchical analysis in an IID framework. We compare our method with the current state-of-the-art and show results with lesser artifacts.
			Finally, we highlight that proper choice and encoding of prior knowledge can produce
			competitive results compared to end-to-end deep learning IID methods, signifying the
			importance of such priors. We believe that the insights and techniques presented in this
			paper would be useful in the future IID research.</i></p>
<pre><code>@inproceedings{DBLP:conf/bmvc/SainiN18,
author    = {Saurabh Saini and P. J. Narayanan},
title     = {Semantic Priors for Intrinsic Image Decomposition},
booktitle = {British Machine Vision Conference 2018, {BMVC} 2018, Northumbria University, Newcastle, UK, September 3-6, 2018},
pages     = {154},
year      = {2018},
url       = {http://bmvc2018.org/contents/papers/0796.pdf},}
</code></pre>
	            </div>
        	    </td>
	        </tr>

		<!-- WACV 2018 -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/3dcap.png"><img src="./data/images/3dcap.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="./data/docs/08354153.pdf" id="WACV18_pdf">
			<heading>Human Shape Capture and Tracking at Home.</heading></a><br>
		<a href="https://researchweb.iiit.ac.in/~gaurav.mishra/">Gaurav Mishra</a>, Saurabh Saini, <a href="https://scholar.google.co.in/citations?user=pDvg7X4AAAAJ&hl=en">Kiran Varanasi</a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>WACV</em>, 2018  <b> (Oral) </b> 
        	      </p>
	              <div class="paper" id="WACV18">
        	        <a href="javascript:toggleblock('WACV18_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('WACV18')" class="togglebib">bibtex</a> | 
			<a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/smpltracking">webpage</a>
			<p align="justify" style="font-size:12px;"> <i id="WACV18_abs">
			Human body tracking typically requires specialized capture set-ups. Although pose tracking is available in consumer devices like Microsoft Kinect, it is restricted to stick
			figures visualizing body part detection. In this paper, we propose a method for full 3D human body shape and motion capture of arbitrary movements from the depth channel
			of a single Kinect, when the subject wears casual clothes. We do not use the RGB channel or an initialization procedure that requires the subject to move around in front of the
			camera. This makes our method applicable for arbitrary clothing textures and lighting environments, with minimal subject intervention. Our method consists of 3D surface
			feature detection and articulated motion tracking, which is regularized by a statistical human body model. We also propose the idea of a Consensus Mesh (CMesh) which is the
			3D template of a person created from a single view point. We demonstrate tracking results on challenging poses and argue that using CMesh along with statistical body models
			can improve tracking accuracies. Quantitative evaluation of our dense body tracking shows that our method has very little drift which is improved by the usage of CMesh.
        	        </i></p>
<pre><code>@inproceedings{DBLP:conf/wacv/MishraSVN18,
author    = {Gaurav Mishra and Saurabh Saini and Kiran Varanasi and P. J. Narayanan},
title     = {Human Shape Capture and Tracking at Home},
booktitle = {2018 {IEEE} Winter Conference on Applications of Computer Vision, {WACV} 2018, Lake Tahoe, NV, USA, March 12-15, 2018},
pages     = {390--399},
year      = {2018},
doi       = {10.1109/WACV.2018.00049},}
</code></pre>
	            </div>
        	    </td>
	        </tr>


		<!-- ICVGIP -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/6_Outdoor3.jpg"><img src="./data/images/6_Outdoor3.jpg" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="./data/docs/a88-saini.pdf" id="ICVGIP16_pdf">
			<heading>Intrinsic image decomposition using focal stacks.</heading></a><br>
			Saurabh Saini, <a href="https://researchweb.iiit.ac.in/~parikshit.sakurikar/">Parikshit Sakurikar </a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>ICVGIP</em>, 2016  <b> (Oral) </b>
        	      </p>
	              <div class="paper" id="ICVGIP16">
        	        <a href="javascript:toggleblock('ICVGIP16_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('ICVGIP16')" class="togglebib">bibtex</a> |
			<a href="https://github.com/sophont01/fStackIID">code</a>
			<p align="justify" style="font-size:12px;"> <i id="ICVGIP16_abs">
			In this paper, we presents a novel method (RGBF-IID) for intrinsic image decomposition of a wild scene without any restrictions on the complexity, illumination or scale of the
			image. We use focal stacks of the scene as input. A focal stack captures a scene at varying focal distances. Since focus depends on distance to the object, this representation has
			information beyond an RGB image towards an RGBD image with depth. We call our representation an RGBF image to highlight this. We use a robust focus measure and generalized
		       	random walk algorithm to compute dense probability maps across the stack. These maps are used to define sparse local and global pixel neighbourhoods, adhering to the structure
		       	of the underlying 3D scene. We use these neighbourhood correspondences with standard chromaticity assumptions as constraints in an optimization system. We present
			our results on both indoor and outdoor scenes using manually captured stacks of random objects under natural as well as artificial lighting conditions. We also test our system on
			a larger dataset of synthetically generated focal stacks from NYUv2 and MPI Sintel datasets and show competitive performance against current state-of-the-art IID methods that
			use RGBD images. Our method provides a strong evidence for the potential of RGBF modality in place of RGBD in computer vision
			</i></p>
<pre><code>@inproceedings{DBLP:conf/icvgip/SainiSN16,
author    = {Saurabh Saini and Parikshit Sakurikar and P. J. Narayanan},
title     = {Intrinsic image decomposition using focal stacks},
booktitle = {Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing, {ICVGIP} 2016, Guwahati, Assam, India, December 18-22, 2016},
pages     = {88:1--88:8},
year      = {2016},
url       = {https://doi.org/10.1145/3009977.3010046},
doi       = {10.1145/3009977.3010046},}
</code></pre>
	            </div>
        	    </td>
	        </tr>
		

		<!-- ICVGIP Aditya -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/tag2vec.png"><img src="./data/images/tag2vec.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="./data/docs/a94-singh.pdf" id="ICVGIP16A_pdf">
			<heading>Learning to hash-tag videos with Tag2Vec</heading></a><br>
			<a href="https://cvit.iiit.ac.in/people/ms-by-research/ms-students/aditya-singh">Aditya Singh</a>, Saurabh Saini, <a href="http://rajvishah.weebly.com/">Rajvi Shah</a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>ICVGIP</em>, 2016  <b> (Oral) </b>
        	      </p>
	              <div class="paper" id="ICVGIP16A">
        	        <a href="javascript:toggleblock('ICVGIP16A_abs')">abstract</a> |
                	<a shape="rect" href="javascript:togglebib('ICVGIP16A')" class="togglebib">bibtex</a> | 
			<a href="https://arxiv.org/abs/1612.04061">arxiv</a>
			<p align="justify" style="font-size:12px;"> <i id="ICVGIP16A_abs">
			User-given tags or labels are valuable resources for semantic understanding of visual media such as images and videos. Recently, a new type of labeling mechanism known as hash-tags
		       	have become increasingly popular on social media sites. In this paper, we study the problem of generating relevant and useful hash-tags for short video clips. Traditional data-driven
		       	approaches for tag enrichment and recommendation use direct visual similarity for label transfer and propagation. We attempt to learn a direct low-cost mapping from
		       	video to hash-tags using a two step training process. We first employ a natural language processing (NLP) technique, skip-gram models with neural network training to
			learn a low-dimensional vector representation of hash-tags (Tag2Vec) using a corpus of ∼ 10 million hash-tags. We then train an embedding function to map video features to
 			the low-dimensional Tag2vec space. We learn this embedding for 29 categories of short video clips with hash-tags. A query video without any tag-information can then be 
			directly mapped to the vector space of tags using the learned embedding and relevant tags can be found by performing a simple nearest-neighbor retrieval in the Tag2Vec space. We
			validate the relevance of the tags suggested by our system qualitatively and quantitatively with a user study.
			</i></p>
<pre><code>@inproceedings{DBLP:conf/icvgip/SinghSSN16,
author    = {Aditya Singh and Saurabh Saini and Rajvi Shah and P. J. Narayanan},
title     = {Learning to hash-tag videos with Tag2Vec},
booktitle = {Proceedings of the Tenth Indian Conference on Computer Vision, Graphics and Image Processing, {ICVGIP} 2016, Guwahati, Assam, India, December 18-22, 2016},
pages     = {94:1--94:8},
year      = {2016},
url       = {https://doi.org/10.1145/3009977.3010035},
doi       = {10.1145/3009977.3010035},}
</code></pre>
	            </div>
        	    </td>
	        </tr>


		<!-- GCPR 2016 -->
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/gcpr.png"><img src="./data/images/gcpr.png" width="100%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top">
              		<p  style="margin-top:20px;"><a href="https://cvit.iiit.ac.in/images/ConferencePapers/2016/GCPR2016.pdf" id="GCPR16_pdf">
			<heading>From Traditional to Modern : Domain Adaptation for Action Classification in Short Social Video Clips</heading></a><br>
			<a href="https://cvit.iiit.ac.in/people/ms-by-research/ms-students/aditya-singh">Aditya Singh</a>, Saurabh Saini, <a href="http://rajvishah.weebly.com/">Rajvi Shah</a>, <a href="https://faculty.iiit.ac.in/~pjn/">P. J. Narayanan</a><br>
			<em>GCPR</em>, 2016 <b> (Oral) </b>
        	      </p>
	              <div class="paper" id="GCPR16">
        	        <a href="javascript:toggleblock('GCPR16_abs')">abstract</a> | 
                	<a shape="rect" href="javascript:togglebib('GCPR16')" class="togglebib">bibtex</a> | 
			<a href="http://arxiv.org/abs/1610.05613">arxiv</a>
			<p align="justify" style="font-size:12px;"> <i id="GCPR16_abs">
			Short internet video clips like vines present a significantly wild distribution compared to traditional video datasets. 
			In this paper, we focus on the problem of unsupervised action classification in wild vines using traditional la- beled datasets.
		       	To this end, we use a data augmentation based simple domain adaptation strategy. We utilize semantic word2vec space as a common subspace to embed
		       	video features from both, labeled source domain and unlabled target domain. Our method incrementally augments the labeled source with target samples
		       	and iteratively modifies the embedding function to bring the source and target distributions together. Additionally, we utilize a multi-modal representation
		       	that incorporates noisy semantic information available in form of hash-tags. We show the effectiveness of this simple adaptation technique on a test set
		       	of vines and achieve notable improvements in performance.
			</i></p>
<pre><code>@inproceedings{DBLP:conf/dagm/SinghSSN16,
author    = {Aditya Singh and Saurabh Saini and Rajvi Shah and P. J. Narayanan},
title     = {From Traditional to Modern: Domain Adaptation for Action Classification in Short Social Video Clips},
booktitle = {Pattern Recognition - 38th German Conference, {GCPR} 2016, Hannover, Germany, September 12-15, 2016, Proceedings},
pages     = {245--257},
year      = {2016},
url       = {https://doi.org/10.1007/978-3-319-45886-1\_20},
doi       = {10.1007/978-3-319-45886-1\_20},}
</code></pre>
	            </div>
        	    </td>
	        </tr>
		  </table>


		<!-- PROJECTS -->

		  <p><br><h1>Projects</h1></p>
		  <table align="center" border="0" cellspacing="0" cellpadding="20">
		    <tr>
	            <td width="33%" valign="top"><a href="./data/images/quatRes_3.png"><img src="./data/images/quatRes_3.png" width="75%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
        	    <td width="67%" valign="top"><br>
					<p><heading>Inverse Light Transport</heading>
						The project aims at studying the feasibility of estimating bounces of light in a scene from a single image by inverting the 
						light transport equation and possibilty of extending the specularity removal methods towards this goal.
					</p>
				</td>
			</tr>
			<tr>
				<td width="33%" valign="top"><a href="./data/images/matNN.jpg"><img src="./data/images/matNN.jpg" width="75%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
				<td width="67%" valign="top"><br><br>
						<p><heading>Neural Rendering for Material Visualization</heading><br>
							The project aims at real-time, raytraced and faithful representation of spatially varying and parametric 
							BRDF models on a shader ball for material visualization and selection.
						</p>
				</td>
			</tr>
			<tr>
				<td width="33%" valign="top"><a href="./data/images/3Dcategorization.png"><img src="./data/images/3Dcategorization.png" width="75%" style="border-style: none; margin-bottom: 20px; margin-top: 20px;"></a>
				<td width="67%" valign="top"><br><br>
						<p><heading>AutoML assisted 3D CNN analysis</heading><br>
							The project aims at analyzing different 3D models like PointNet, VoxelNet, Mutiview CNNs, GraphCNNs etc. 
							using AutoML techniques tworads the goal of better understading the strengths and weaknesses of 3D CNNs for various 3D problems.
						</p>
				</td>
			</tr>	
		</table>
		</div>
		 &nbsp;<br>
		 


    </section>
    <script src="javascripts/scale.fix.js"></script>
    <script xml:space="preserve" language="JavaScript">	hideallbibs();	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('sf_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('IJCVSPIID_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('WACV18_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('ICVGIP16_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('ICVGIP16A_abs');	</script>
    <script xml:space="preserve" language="JavaScript">	hideblock('GCPR16_abs');	</script>
  </body>
</html>
